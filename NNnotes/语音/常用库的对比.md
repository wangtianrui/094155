### 音频读取之librosa & scipy.io.wavfile

```python
# lib读出来的数据是 scipy读出来的 1/32768
y, sr = lib.load(filename, sr=None)
wav_freq, wav_data = sci_wav.read(filename)
plt.subplot(2, 1, 1)
plt.plot(np.arange((len(y))), y)
plt.subplot(2, 1, 2)
plt.plot(np.arange(len(wav_data)), wav_data)
plt.show()
```

[![AZ5dnf.png](https://s2.ax1x.com/2019/03/17/AZ5dnf.png)](https://imgchr.com/i/AZ5dnf)

#### lib

<http://mhy12345.xyz/tutorials/librosa-samples/>

```python
# Feature extraction example
import numpy as np
import librosa

# Load the example clip
y, sr = librosa.load(librosa.util.example_audio_file())

# Set the hop length; at 22050 Hz, 512 samples ~= 23ms
hop_length = 512

# Separate harmonics and percussives into two waveforms
y_harmonic, y_percussive = librosa.effects.hpss(y)

# Beat track on the percussive signal
tempo, beat_frames = librosa.beat.beat_track(y=y_percussive, sr=sr)

# Compute MFCC features from the raw signal
mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)

# And the first-order differences (delta features)
mfcc_delta = librosa.feature.delta(mfcc)

# Stack and synchronize between beat events
# This time, we'll use the mean value (default) instead of median
beat_mfcc_delta = librosa.util.sync(np.vstack([mfcc, mfcc_delta]),
                                    beat_frames)

# Compute chroma features from the harmonic signal
chromagram = librosa.feature.chroma_cqt(y=y_harmonic,
                                        sr=sr)

# Aggregate chroma features between beat events
# We'll use the median value of each feature between beat frames
beat_chroma = librosa.util.sync(chromagram,
                                beat_frames,
                                aggregate=np.median)

# Finally, stack all beat-synchronous features together
beat_features = np.vstack([beat_chroma, beat_mfcc_delta])mel_feat = lib.feature.melspectrogram(y=y, sr=srate, n_fft=n_fft, hop_length=hop_length, n_mels=128)
```

