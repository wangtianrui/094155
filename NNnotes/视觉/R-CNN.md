# RCNN

https://zhuanlan.zhihu.com/p/23006190

### Region CNN

##### 一、这个方法解决了传统目标检测的两个问题：

##### ①速度

```text
经典的目标检测算法是使用滑动窗口依次判断所有可能的区域。而这个方法则是预先提取出一系列较可能是物体的候选区域，之后对这些区域进行判断就Ok
```

##### ②训练集

```text
景点的目标检测算法是在区域中提取人工设定的特征（haar、hog特征https://github.com/wangtianrui/My-notes/blob/master/NNnotes/haar.md）。而该方法需要训练网络来进行特征的提取
```

##### 二、大致流程

* 一张图生成1k-2k个候选区


* 对每个候选区。使用卷积网络提取特征
* 将特征送入SVM分类器，判别类型
* 使用回归器精细修正候选框的位置

##### 三、候选区域的生成

使用了Selective Search方法从一张图生产约2000~3000个候选区域。基本思路：

使用一种过分割手段，将图像分割成小区域

查看现有的区域，合并可能性最高的两个区域，重复知道整张图像合并成一个区域位置

输出所有曾经存在过的区域，即候选区

##### 四、合并规则

优先合并以下四种区域：

* 颜色直方图相近的

* 纹理（梯度直方图）相近的

* 合并后总面积小的

  注意这里合并的“规则”应该比较均匀，避免一个大区域不断地“吃掉”小区域

  > 例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -> abcd-efgh -> abcdefgh。 
  > 不好的合并方法是：ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh。

* 接触面较大

  注意合并后的形状比较“规则”

  > 相较于右边，左边的更好
  >
  > ![](https://img-blog.csdn.net/20160405212106908)
  >
  > ​

上述四条规则只涉及区域的颜色直方图、纹理直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。

##### 五、多样化与后处理

为了尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（三通道）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。

##### 六、特征提取

##### ① 预处理

在使用卷积网络提取特征前将候选区域归一化成同一尺寸227x227

此处需要注意：外扩尺寸大小，reshape时是否保持原比例，对框外区域直接截取还是补灰。都会有一定的影响。

##### ②预训练

​	将cnn网络最后的FC层改为class_number+1神经元数（class_number个种类+1个背景）

如：4096->11 （训练了10分类，加上一个背景类，就是11）

##### ③位置精修

​	目标检测问题的衡量标准是重叠面积：许多看似准确的结果往往因为候选框不够精准，重叠面积很小。所以需要一个位置精修步骤。对每一类目标，使用一个线性回归器进行精修。



##### 疑惑点：

CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？

这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm



> 再回顾总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。