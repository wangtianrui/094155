## 所有提到的测试代码均在https://github.com/wangtianrui/MachineLearningStudy/blob/master/apitest/one.py

* ## 交叉熵

### 交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：

![](https://pic4.zhimg.com/80/v2-8cd1c428c096608e38c46dc0b5433798_hd.jpg)

>  ​							其中，y 是我们预测的概率分布, y’ 是实际的分布

>​		特点：1.交叉熵是正的，2.当所有输入x的输出都能接近期望输出y的话，交叉熵的值将会接近 0。

>​		优点：

>​		对比于二次代价函数：以下为二次代价函数参数更新的公式:

​		                				![](https://pic1.zhimg.com/80/v2-527975d183e4d89e4b918086e4d9b75d_hd.jpg)

![](https://pic1.zhimg.com/80/v2-4d7a9b7d68a8893ddc6d0d75ba082fe9_hd.jpg)

> ​	a 是 神经元的输出，其中 a = σ(z)， z = wx + b，一般激活函数在末和头的导数都特别小，如：     

>       sigmoid，所以如果用二元代价函数来做代价函数的话，会出现开头和末尾训练速度特别慢的情况

​	 					![](https://pic3.zhimg.com/80/v2-e380672ebecf809c6edc79a9f692804b_hd.jpg)

​		

> ​	对于cross-entropy来说:

​		![](https://pic4.zhimg.com/80/v2-c5cfad423e9cdf6e3348411b8cebad34_hd.jpg)

>  	偏导后：

![](https://pic2.zhimg.com/80/v2-8418217157122a69cb3f752ea1af4bb4_hd.jpg)

> ​	可以发现。偏导结果是与“距离值”有关的，所以会比二次代价函数好

* ## local response normalizatio

![](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170713145228303.png?raw=true)



![](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170713162906129.png?raw=true)



参考上图。Local Response Normalization是以point为中心在channel维度上进行一个归一化处理（beta和alpha不同情况不同）

```
调整beta=》beta则是调整k与alpha整体作用的参数,只要k大于1，永远是大于1的值，所以是调小
 alpha=》调整极差（越小极差越大,但是极差值不会超过原本输入值）
个人理解 k（biase）值的设置是为了确定是让原本值调大调小的参数，一般设为1，这里讲一下为什么设为1，个人理解
是如果k为1，那么通过调整alpha和beta值就可以来对整体“极差”进行调整（alpha值很小，那么“处理”的作用就会比较小,如果alpha比较大那么就会将整体数值进行调小），
综上可大致得出结论：lrn操作通过调小“极差”（也就是将影响作用非常大的特征的数值进行调小）的操作，来防止过拟合的发生
```

详细情况请参考测试代码的testLRN函数

* ## dropout

![](https://github.com/wangtianrui/My-notes/blob/master/pictures/7e31586d15d887ae0901452e2e1b1c6cb94f882e.png?raw=true)

Dropout的思想是训练整体DNN，并平均整个集合的结果，而不是训练单个DNN。DNNs是以概率P舍弃部分神经元，其它神经元以概率q=1-p被保留，舍去的神经元的输出都被设置为零。输入参数中保留keep_pro参数，未保留参数致0，保留参数 输出为 input/keep_pro，部分输入舍弃，同时“强化”其他input，个人理解：调0就不用解释是干啥了，调大参数的不仅是为了“加快”训练速度	还一定程度上保证了“和”的稳定性，相当于是把周围的值给“拿”过来

详细函数情况参考testDropOut函数

* ## batch normalization

  > 为什么要标准化？因为当我们的x（输入）到达一定值（较大值）时会接近于1，也就是说：轻轻打你一下和重重打你一下，但是在你看来两次打你效果是一样的，所以感知网络就明显会出现问题，那么如果我们调整了分布，就可以将我们从激活函数得到的结果分布更加均匀（这里的均匀不是指值相似，而是让每一个值都能“传递下去”）
  >
  > 减均值->zscore->白化可以逐级提升随机初始化的权重对数据分割的有效性，还可以降低overfit的可能性。我们都知道，现在的神经网络的层数都是很深的，如果我们对每一层的数据都进行处理，训练时间和overfit程度是否可以降低呢？
  >
  > https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/

  ![](![20170721163014837.png](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170721163014837.png?raw=true))

  传统的神经网络，只是在将样本xx输入输入层之前对xx进行标准化处理（减均值，除标准差），以降低样本间的差异性。BN是在此基础上，不仅仅只对输入层的输入数据xx进行标准化，还对每个隐藏层的输入进行标准化

  ![](![20170721163449112.png](https://github.com/wangtianrui/My-notes/blob/master/pictures/20170721163449112.png?raw=true))

  ![](https://github.com/wangtianrui/My-notes/blob/master/pictures/TIM%E5%9B%BE%E7%89%8720180328214911.png?raw=true)

  BN就是在神经网络的训练过程中对每层的输入数据加一个标准化处理。这里更正一下 。b的作用不会被抵消，因为算变准差的时候b是有效的，所以要加上biass

  参考讲解：http://www.360doc.com/content/17/0411/13/10408243_644654557.shtml

  https://blog.csdn.net/gqixf/article/details/78872934   这个详细些

  代码参考testBN

* ##标准化

  ```txt
  标准化的目的是为了降低不同特征的不同范围的取值对于模型训练的影响；比如对于同 一个特征，不同的样本的取值可能会相差的非常大，那么这个时候一些异常小或者异常 大的数据可能会误导模型的正确率；另外如果数据在不同特征上的取值范围相差很大， 那么也有可能导致最终训练出来的模型偏向于取值范围大的特征，特别是在使用梯度下 降求解的算法中；通过改变数据的分布特征，具有以下两个好处：1. 提高迭代求解的收 敛速度；2. 提高迭代求解的精度
  归一化对于不同特征维度的伸缩变换的主要目的是为了使得不同维度度量之间特征具有 可比性，同时不改变原始数据的分布(相同特性的特征转换后，还是具有相同特性)。和 标准化一样，也属于一种无量纲化的操作方式。
  正则化则是通过范数规则来约束特征属性，通过正则化我们可以降低数据训练处来的模 型的过拟合可能，和之前在机器学习中所讲述的L1、L2正则的效果一样。在进行正则化 操作的过程中，不会改变数据的分布情况，但是会改变数据特征之间的相关特性。 备注：广义上来讲，标准化、区间缩放法、正则化都是具有类似的功能。在有一些书籍 上，将标准化、区间缩放法统称为标准化，把正则化称为归一化操作。
  简单来说
  标准化会改变数据的分布情况，归一化不会，标准 化的主要作用是提高迭代速度，降低不同维度之间影响权重不一致的问题。
  ```

* ## 特征选择

  > https://www.jianshu.com/p/ebc04e52d7c7

  ```txt
  在选择模型的过程中，通常从两方面来选择特征：
  特征是否发散：如果一个特征不发散，比如方差解决于0，也就是说这样的特征对于样本的 区分没有什么作用。
  特征与目标的相关性：如果与目标相关性比较高，应当优先选择。
  特征选择的方法主要有以下三种：

  Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择 阈值的个数，从而选择特征；常用方法包括方差选择法、相关系数法、卡方检验、 互信息法等。
  Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征或 者排除若干特征；常用方法主要是递归特征消除法。
  Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征 的权重系数，根据系数从大到小选择特征；常用方法主要是基于惩罚项的特征选择 法。

  ```

* ### 多通道卷积

![](http://img.blog.csdn.net/20170318112222765)

```txt
输入图像layer m-1有4个通道，同时有2个卷积核w1和w2。对于卷积核w1，先在输入图像4个通道分别作卷积，再将4个通道结果加起来得到w1的卷积输出；卷积核w2类似。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！特殊的，对于多通道做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起。这样做的结果就是把原图像中本来各个独立的通道“联通”在了一起。
```

