## 问题集合 （这里备份一下，自己也好查，所有均自己的理解和总结，如果发现有误请在issus里为我提出。万分感谢）

### 问题： 就像识别手写数字这个程序，如果按你说的一般99%的时间都在调参。可具体到怎么调参有什么思路和技巧吗
> 调参突破口

##### 优先考虑：

* you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. （摘自cs231n assignmen-t5)  


* 卷积核 ： 大小、核数、步长 ， 如果图像是奇数边长（基本没人会做这样的数据集）的时候还可以考虑一下Padding
* 池化：ksize 和 步长 ， 池化类型
* 卷积和池化的层数
* 全连接：层数、宽度
* dropout率
* 加bn操作
* LRN操作
* 初始化方式和stddev
* PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.

##### 次要考虑（一般用到细节优化）:

* Learning_rate

* 交叉熵那儿是求均值还是求和

* 考虑是否提前中断

* 考虑是否需要在训练过程中更改Learning_rate

##### 最次要（一般不好实现）

* 增大数据集，花更多的人力去做出更多数据集

* 采用留一法等其它数据使用方式

### 问题：梯度消融是因为在全连接层选的不是softmax算法造成的？

> 梯度弥散现象的造型

##### 原因：

* sigmoid函数的局限性

* 全连接最后一层用sigmoid，然后使用二次均值loss计算方式

* 层数过高，没有使用relu

##### 解决方式：

* 使用relu

* cross_entropy

* 使用softmax

### 问题：SPP-NET中对整图卷积后的classify操作是怎么进行的？

> 网络理解

详细参考spp-net笔记第二张图片下的括号内的描述



### 根据可视化进行优化

看看上面的可视化，我们发现损失是线性递减的，这似乎表明学习速率可能太低了。此外，培训和验证的准确性之间没有差距，这表明我们使用的模型的容量很低，而且我们应该增加它的大小。另一方面，对于一个非常大的模型，我们期望看到更多的过度拟合，这将显示出在训练和验证准确性之间的巨大差距。（摘自cs231n assignmen-t5)  

* 换而言之不能盲目地直接降低学习率，这样不容易跳出局部最小。！